\section{Comparison with Other Inference Methods}
\label{sec:comparison}

In the following section, we compare the results of the Bayesian method with other classifiers commonly used for this same problem.

This classified creates a proper baseline for other comparisons. These will be used in the dataset $G = \left< V, E \right>$ with the bank data contained in $B^{\train}$, and will be used eihter to provite information about the users contained in $B^{\test}$ (the \emph{Outer Graph}), or the ones in $\Upsilon$ (the \emph{Inner Graph}). Those two sets of features are described with more detail in Section~\ref{subsec:train_test_split} and Section~\ref{subsec:rebalancing_labels}.

For simplicity sake, the descriptions of the algorithm will refer to the \emph{Testing Set} as $\Upsilon$ regardless of whether it's using a subset or the data or not.

Table~\ref{tab:comparison} compares the results of these methods with the \emph{Bayesian Algorithm} described in Section~\ref{sec:inference_methodology} along several metrics.

\subsection{Random Selection}
\label{subsec:random_selection}

For completeness sake we created a random ``dummy'' classifier which randomly chooses the socioeconomic index of each user.

\begin{equation}
\label{eq:random}
\begin{aligned}
	P \left( v \in H_1 \right) &= \sfrac{1}{2} \\
	P \left( v \in H_2 \right) &= \sfrac{1}{2}
\end{aligned}
\end{equation}

\subsection{Majority Voting}
\label{subsec:majority_voting}

The method of \emph{Majory Voting} is a basic but powerful way of inferring to which category a user belongs. It simply chooses the category of each user $\upsilon \in \Upsilon$ as the category for which the majority of its contacts belong.

In case of a tie (which happens often when using the \emph{Outer Graph}), the category is chosen randomly.

\begin{equation}
\label{eq:majority_voting}
\begin{aligned}
	\contacts^{\low}_{\upsilon} &> \contacts^{\high}_{\upsilon} \implies \upsilon \in H_1 \\
	\contacts^{\low}_{\upsilon} &< \contacts^{\high}_{\upsilon} \implies \upsilon \in H_2 \\
	\contacts^{\low}_{\upsilon} &= \contacts^{\high}_{\upsilon} \implies
	\begin{cases}
		\upsilon \in H_1 \  \text{with probability} \ \sfrac{1}{2} \\
		\upsilon \in H_2 \  \text{with probability} \ \sfrac{1}{2}
	\end{cases}
\end{aligned}
\end{equation}

\subsection{Averaging Contacts' Incomes}

\todo{Complete this}

\subsection{Methods Based in Machine Learning}
\label{subsec:methods_ml}

The following methods use commonly used supervised Machine Learning algorithms described in Section~\ref{subsec:supervised_machine_learning} used with many similar \emph{Feature Extraction} methods on $G$.

Each method is identified with an integer $n \in \mathbb{N}$ which represents the size of the \emph{Ego Network} used to get information for each node. Additionally, some categories have \emph{Category Data} from $B^{\train}$ to help identify the network.

Additionally, the features of each method are merged with the ones from the immediate predecessor methods, as shown in the graph in Figure~\ref{fig:mlrelationships}.

\begin{figure}
\centering
\includegraphicsmaybe{figures/mlrelationships.png}
\caption{Relationships between the \emph{Feature Extraction} methods of Section~\ref{subsec:methods_ml}}
\label{fig:mlrelationships}
\end{figure}

\subsection{User Data --- Level 0}
\label{subsec:user_data}

The features on the graph $G = \left< V, E \right>$ can be accumulated for all users in a manner similar to the one in Section~\ref{subsec:feature_accumulation} using the data generated in Equation~\ref{eq:graphconstruction}. However, this time we aren't constrained either by having only users which have contacts with other users with known \emph{Income Category}, nor with having to have only two features for each category (which was necessary due to using the \emph{Beta Distribution}).

This way, it's possible to accumulate features for each user $v \in V$ in the manner shown by Equation~\ref{eq:user_data}.

\begin{equation}
\label{eq:user_data}
\begin{gathered}
\begin{aligned}
\incalls_v &= \sum_{\substack{e \in E \\ e_d = v}} \calls_e &
\outcalls_v &= \sum_{\substack{e \in E \\ e_o = v}} \calls_e \\
\intime_v &= \sum_{\substack{e \in E \\ e_d = v}} \etime_e &
\outtime_v &= \sum_{\substack{e \in E \\ e_o = v}} \etime_e \\
\insms_v &= \sum_{\substack{e \in E \\ e_d = v}} \sms_e &
\outsms_v &= \sum_{\substack{e \in E \\ e_o = v}} \sms_e \\
\end{aligned} \\
\begin{aligned}
\incontacts_v &= \left| \left\{ e \in E \mid e_d = v \right\} \right| \\
\outcontacts_v &= \left| \left\{ e \in E \mid e_o = v \right\} \right|
\end{aligned}
\end{gathered}
\end{equation}

These features will be referred as the \emph{User Data} of user $v$.

There features contain information about the \emph{Neighbourhood} of $\upsilon$, also referred to as the \emph{Ego Network of Distance 1}. This \emph{Neighbourhood} can be formally defined as in Equation~\ref{eq:neighbourhood}.

\begin{equation}
\label{eq:neighbourhood}
\neigh \left( \upsilon \right) = \left\{ e_o \mid e \in E \and e_d = \upsilon \right\} \cup \left\{ e_d \mid e \in E \and e_o = \upsilon \right\}
\end{equation}

\subsection{Categorical User Data --- Level 0'}
\label{subsec:categoricaluserdata}

A possible featureset consists of using of separating the data of the neighbourhood of each user $\upsilon \in \Upsilon$ into two disjoint groups, $L_{\upsilon}$ and $K_{\upsilon}$, which contain the neighours of $\upsilon$ in the \emph{Low Income} and \emph{High Income} categories of income respectively\footnotemark{}.

\footnotetext{Note that, since not all users have banking information, there may be nodes in the neighbourhood of $\upsilon$ which don't belong to either $L_{\upsilon}$ or $K_{\upsilon}$.}

\begin{equation}
\label{eq:cud_categories}
\begin{aligned}
	L_{\upsilon} &= H_1 \cap \neigh \left( \upsilon \right) \\
	K_{\upsilon} &= H_2 \cap \neigh \left( \upsilon \right)
\end{aligned}
\end{equation}

Having these groups it's possible to define a set of features similar to the one in Section~\ref{subsec:user_data}, where each feature is separated by the category of the neighbour. Equation~\ref{eq:matcatuserdata} contains the names of the new features.

\begin{equation}
\label{eq:matcatuserdata}
	\begin{Bmatrix} in \\ out \end{Bmatrix}
	\times
	\begin{Bmatrix} calls \\ time \\ sms \\ contacts \end{Bmatrix}
	\times
	\begin{Bmatrix} low \\ high \end{Bmatrix}
\end{equation}

Equations~\ref{eq:categoricaluserdata} and~\ref{eq:categoricaluserdata2} contain the way to calculate those features. Since the number of individual features is high and the formulas are similar and repetitive, some generalizations were added for the simplest ones.

\begin{equation}
\label{eq:categoricaluserdata}
\begin{gathered}
	\left( \forall \varpi \in \left\{ \calls, \etime, \sms \right\} \right) \\
\begin{aligned}
	\underline{\text{in}{\varpi}\text{low}}_v = \sum_{\substack{e \in E \\ e_d \in L_v \\ e_o = v}} &\varpi_e &
	\underline{\text{in}{\varpi}\text{high}}_v = \sum_{\substack{e \in E \\ e_d \in K_v \\ e_o = v}} &\varpi_e \\
	\underline{\text{out}{\varpi}\text{low}}_v = \sum_{\substack{e \in E \\ e_o \in L_v \\ e_d = v}} &\varpi_e &
	\underline{\text{out}{\varpi}\text{high}}_v = \sum_{\substack{e \in E \\ e_o \in K_v \\ e_d = v}} &\varpi_e \\
\end{aligned}
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:categoricaluserdata2}
\begin{aligned}
	\incontactslow_v   &= \left| \left\{ e \in E \mid e_d = v \land e_o \in L_v \right\} \right| \\
	\incontactshigh_v  &= \left| \left\{ e \in E \mid e_d = v \land e_o \in K_v \right\} \right| \\
	\outcontactslow_v  &= \left| \left\{ e \in E \mid e_o = v \land e_d \in L_v \right\} \right| \\
	\outcontactshigh_v &= \left| \left\{ e \in E \mid e_o = v \land e_d \in K_v \right\} \right|
\end{aligned}
\end{equation}

Unlike the features in Section~\ref{subsec:user_data}, and like the method presented in Section~\ref{sec:inference_methodology}, the features in this section will be be different when testing between the nodes of $B_{\test}$ (the \emph{Outer Graph}) and $\Upsilon$ (the \emph{Inner Graph}).

\subsection{Higher Order User Data}
\label{subsec:higherorderuserdata}

Where \emph{User Data} contains data directly about the neighbours of each user, we can define \emph{User Data of Order $1$}, as the user data variables about the edges of the neighbours of each user $u$ where $u$ isn't any of the endpoints. In the case of cumulative data (calls, time, and SMS), this is equal to the sum of the \emph{User Data} of the neighbours of $u$ minus the \emph{User Data} os $u$ (since we aren't counting ``inner'' edges). This isn't true in the case of the contacts, since two neighbours of $u$ may have a contact in common, which should be counted only once.

Additionally, for any $n \in \mathbb{N}$, we can inductively define the \emph{User Data of Order $n$} as the user data of the nodes at distance $n$ of a certain node.

\subsection{Validation Metrics}
\label{subsec:validationmetrics}
There are several validation metrics used for each method.

\begin{description}
	\item[Accuracy] as described in Subsection~\ref{subsec:accuracy}, which measures the general performance of this method.
	\item[Precision] as described in Subsection~\ref{subsec:precisionrecall}, which measures the performance regarding the positive instances found by this method.
	\item[Recall] as described in Subsection~\ref{subsec:precisionrecall}, which measures the performance regarding the positive instances in the dataset.
	\item[Area Under the Curve] as described in Subsection~\ref{subsec:auc}, which measures the general performance disregarding which threshold is used.
	\item[$\mathbf{F_1}$ Score] as described in Subsection~\ref{subsec:fmeasure} which is generalized score balancing Precision and Recall.
	\item[$\mathbf{F_4}$ Score] as described in Subsection~\ref{subsec:fmeasure}, which gives more weight to the Recall. This is usually wanted since the ultimate practical objective of this study is to find wealthier people, even if the result has low Precision.
	\item[Time] can be used to break ties between similar models.
\end{description}

\subsection{Feature Extraction}
\label{subsec:featureextraction}
Using the previous graph, we can create features separated into different levels.

\begin{enumerate}
	\item[0] \emph{Local Features} as described in Section~\ref{subsec:user_data}.
	\item[0.5] \emph{Classified Local Features} as described in Section~\ref{subsec:categoricaluserdata} (using only the adjacency information of the training set $T$), joined with the data on the previous item. There are two separate sets of sampels used for experiments on this level.
	\begin{description}
		\item[Outer Nodes] which contain all samples in $S$.
		\item[Innter Nodes] which only contain samples in the testing set $S$ which have at least a neighbour in the training set $T$.
	\end{description}
	\item[1] \emph{Neighbour Features}, as described in Section~\ref{subsec:higherorderuserdata} for \emph{User Data of Order 1}, joined with data on \textbf{Level 0}.
	\item[1.5] \emph{Classified Neighbouring Features}, which combine the feature extraction methods seen in Section~\ref{subsec:categoricaluserdata} and Section~\ref{subsec:higherorderuserdata}, and separate the accumulation of the \emph{User Data} of a node's neighbours along categorical features. This data is also joined with all previous levels, and discriminated between sets of \emph{Outer Nodes} and \emph{Inner Nodes}
	\item[$\geq 2$] The method described in Section~\ref{subsec:higherorderuserdata}, with or without classification by the type of the other endpoint of the link. In this section, we'll prove that the accuracy of the methods before this one are similar enough to the ones in this level so that sacrificing speed is not worth the new features.
\end{enumerate}

\begin{table}
\centering
\begin{tabular}{>{\bfseries}l c c c c c c c}
\toprule
\textbf{Level} & 0 & \multicolumn{2}{c}{0.5} & 1 & \multicolumn{2}{c}{1.5} & 2 \\
\cmidrule(lr){3-4} \cmidrule(lr){6-7}
\textbf{Dataset} &   & inner     &    outer    &   & inner     &    outer    &   \\
\midrule
Features &\num{1234567}&\num{1234567}&\num{1234567}&\num{1234567}&\num{1234567}&\num{1234567}&\num{1234567}\\
Samples  &\num{1234567}&\num{1234567}&\num{1234567}&\num{1234567}&\num{1234567}&\num{1234567}&\num{1234567}\\
\bottomrule
\end{tabular}
\caption{Size of the datasets used for testing for each feature extraction method}
\label{tab:datasettable}
\todo{Change placeholders for actual numbers}
\end{table}

These datasets contain differing numbers of features and samples, as described in Table~\ref{tab:datasettable}.

\subsection{Results}

Applying the features from Section~\ref{subsec:featureextraction} to the \emph{Random Forest} model described in Section~\ref{subsec:randomforest} and the \emph{Logistic Regression} model described in Section~\ref{subsec:logisticregression}. Prior to applying each model, a \emph{Grid Search} was made with the parameter $C$ in the case of the \emph{Logistic Regression} as shown by Equation~\ref{eq:gridsearch}, and the \emph{Criterion} used in the \emph{Random Forest}, and the hyperparameter which results in the most \emph{Accuracy} after 5-fold cross validation was used.

\begin{equation}
\label{eq:gridsearch}
\begin{split}
C &\in \left\{ 0.01, 0.1, 1, 10, 100 \right\} \\
\operatorname{Criterion} &\in \left\{ \text{Gini}, \text{Entropy} \right\}
\end{split}
\end{equation}

The results of the \emph{Grid Search} are presented in Table~\ref{tab:gridsearch}, while the different metrics of the result are presented in Table~\ref{tab:comparison}.

\begin{table}
\centering
\begin{tabular}{>{\bfseries}l l @{\hskip 2em} r r}
\toprule
Level & Dataset & $C$ (LR) & Criterion (RF) \\
\midrule

0 & & \num{10} & Entropy \\ [1.5ex]

\multirow{2}{*}{0.5} & Outer & \num{0.01} & Gini \\
& Inner & \num{1} & Gini \\ [1.5ex]

1 & & \num{100} & Entropy \\ [1.5ex]

\multirow{2}{*}{1.5} & Outer & \num{100} & Entropy \\
& Inner & \num{100} & Entropy \\
\bottomrule

\end{tabular}
\caption{Best hyperparameters for each group of features in each model used}
\label{tab:gridsearch}
\end{table}

\begin{table}
\begin{tabular*}{\textwidth}{>{\bfseries}l l l @{\extracolsep{\fill}} r r r r r r r}
\toprule
Level & Dataset & Method & Accuracy & Precision & Recall & AUC & F\textsubscript{1}-score & F\textsubscript{4}-score & Time \\
\midrule

\multirow{2}{*}{0}
& & LR & \num{0.541} & \num{0.580} & \num{0.299} & \num{0.541} & \num{0.395} & \num{0.308} & \SI{2.647}{\second} \\
& & RF & \num{0.541} & \num{0.542} & \num{0.528} & \num{0.541} & \num{0.535} & \num{0.528} & \SI{11.396}{\second} \\
\midrule

\multirow{4}{*}{0.5}
& \multirow{2}{*}{Outer} & LR &\num{0.570} & \num{0.749} & \num{0.212} & \num{0.570} & \num{0.330} & \num{0.221} & \SI{3.727}{\second} \\
& & RF &\num{0.567} & \num{0.572} & \num{0.533} & \num{0.567} & \num{0.552} & \num{0.535} & \SI{8.528}{\second} \\
\cmidrule{2-10}
& \multirow{2}{*}{Inner} & LR &\num{0.720} & \num{0.747} & \num{0.842} & \num{0.675} & \num{0.792} & \num{0.836} & \SI{1.005}{\second} \\
& & RF &\num{0.700} & \num{0.738} & \num{0.817} & \num{0.658} & \num{0.775} & \num{0.812} & \SI{2.146}{\second} \\
\midrule

\multirow{2}{*}{1}
& & LR & \num{0.552} & \num{0.597} & \num{0.326} & \num{0.553} & \num{0.422} & \num{0.335} & \SI{5.255}{\second} \\
& & RF & \num{0.572} & \num{0.580} & \num{0.527} & \num{0.572} & \num{0.552} & \num{0.530} & \SI{22.523}{\second} \\
\midrule

\multirow{4}{*}{1.5}
& \multirow{2}{*}{Outer} & LR & \num{0.598} & \num{0.726} & \num{0.358} & \num{0.606} & \num{0.479} & \num{0.369} & \SI{22.851}{\second} \\
& & RF & \num{0.723} & \num{0.747} & \num{0.852} & \num{0.675} & \num{0.796} & \num{0.845} & \SI{4.780}{\second} \\
\cmidrule{2-10}
& \multirow{2}{*}{Inner} & LR & \num{0.643} & \num{0.668} & \num{0.617} & \num{0.644} & \num{0.642} & \num{0.620} & \SI{21.498}{\second} \\
& & RF & \num{0.735} & \num{0.766} & \num{0.838} & \num{0.697} & \num{0.800} & \num{0.834} & \SI{6.706}{\second} \\
\midrule

\multirow{2}{*}{2}
& & LR & \num{0} & \num{0} & \num{0} & \num{0} & \num{0} & \num{0} & \SI{0}{\second} \\
& & RF & \num{0} & \num{0} & \num{0} & \num{0} & \num{0} & \num{0} & \SI{0}{\second} \\
\bottomrule

\end{tabular*}
\caption{Results of running dataset with different feature extraction methods}
\label{tab:comparison}
\todo{Add level $\geq 2$}
\end{table}
