In the following section, we compare the results of the Bayesian method with other classifiers.

\begin{figure}
	\begin{tabularx}{\textwidth}{>{\bfseries}X c c c c c c}
		\toprule
		\textbf{Classifier} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{AUC} & \textbf{$F_1$}-\textbf{score} & \textbf{$F_4$}-\textbf{score} \\
		\midrule
		Método Bayesiano & 0 & 0 & 0 & 0 & 0 & 0 \\
		\midrule
		Random Selection & 0.500 & 0.510 & 0.501 & 0.500 & 0.529 & 0.504 \\
		Majority Voting & 0 & 0 & 0 & 0 & 0 & 0 \\
		\midrule
		Naïve Bayes & 0.569 & 0.550 & 0.711 & 0.550 & 0.649 & 0.703 \\
		K Nearest Neighbors\footnote{Selecting $K = 9$} & 0.551 & 0.589 & 0.666 & 0.55 & 0.625 & 0.661 \\
		SVM with RBF kernel\footnote{Selecting $C = 100$} & 0.559 & 0.561 & 0.947 & 0.571 & 0.705 & 0.910 \\
		Logistic Regression\footnote{Selecting $C = 0.01$} & 0.563 & 0.563 & 0.991 & 0.569 & 0.718 & 0.949 \\
		Stochastic Gradient Descent & 0.557 & 0.575 & 0.861 & 0.516 & 0.679 & 0.833 \\
		Random Forest & 0.570 & 0.571 & 0.638 & 0.668 & 0.606 & 0.673 \\
		\midrule
		Categorized LR & 0 & 0 & 0 & 0 & 0 & 0 \\
		Count Link Model & 0 & 0 & 0 & 0 & 0 & 0 \\
		\bottomrule
	\end{tabularx}
\end{figure}

\subsection{Random Selection}

For completeness sake we created a random classifier for the socioeconomic category of each user.

We applied two other inference methods to the same data and compared their accuracies to our Bayesian model.

\begin{itemize}
	\item \textbf{Random selection} which chooses randomly the category for each user.
	\item \textbf{Majority voting} which decides whether a user is in the high or low income category depending on the category of the majority of its contacts. In case of a tie, the category is chosen randomly.
\end{itemize}

The \emph{accuracy} of the first method is as expected \num{0.5}, while the \emph{accuracy} for majority voting is \num{0.66}.
With the Bayesian method we obtain an \emph{accuracy} of \num{0.71}.

\subsection{Graph Sampling}

The set $B$ is randomly sampled into two disjoint subsets, $T$ and $S$, such that $T \cup S = B$ and $T \cap S = \varnothing$. Additonally, $\left| T \right| = \sfrac{4}{5} \cdot \left| B \right|$ and $\left| S \right| = \sfrac{1}{5} \cdot \left| B \right|$. We use $T$ as the \emph{Training Set} of the data, while $S$ is considered the \emph{Testing Set}. This way, we can be sure of not having overfitting issues when applying \emph{Machine Learning} methods to the data.

\subsection{User Data}
\label{subsec:user_data}

As explained in Section~\ref{subsec:mobiledatasource}, the set $P$ contains \emph{Call Detail Records}. Each element $p \in P$ contains the caller and the callee $\left< p_o, p_d \right>$ and call data (including the call duration $p_s$). Additionally, the set $S$ contains \emph{SMS Records}, which for each element $s \in S$ contain origin and destination data $\left< s_o, s_d \right>$.

For each element of the set of users that belong to the telco $u \in U \subseteq P_o \cup P_d \cup S_o \cup S_d$, it's possible to define the following features.

\begin{equation}
\label{eq:calls}
\begin{split}
\operatorname{incalls}_u  &= \left| \left\{ p \in P \mid p_d = u \right\} \right| \\
\operatorname{outcalls}_u &= \left| \left\{ p \in P \mid p_o = u \right\} \right|
\end{split}
\end{equation}

\begin{equation}
\label{eq:time}
\begin{split}
\operatorname{intime}_u  &= \sum_{\substack{p \in P \\ p_d = u}} \, p_s \\
\operatorname{outtime}_u &= \sum_{\substack{p \in P \\ p_o = u}} \, p_s
\end{split}
\end{equation}

\begin{equation}
\label{eq:sms}
\begin{split}
\operatorname{insms}_u  &= \left| \left\{ s \in S \mid s_d = u \right\} \right| \\
\operatorname{outsms}_u &= \left| \left\{ s \in S \mid s_o = u \right\} \right|
\end{split}
\end{equation}

\begin{equation}
\label{eq:contacts}
\begin{split}
\operatorname{incontacts}_u  &= \left| \left\{ p_o \mid p \in P \and p_d = u \right\} \cup \left\{ s_o \mid s \in S \and s_d = u \right\} \right| \\
\operatorname{outcontacts}_u &= \left| \left\{ p_d \mid p \in P \and p_o = u \right\} \cup \left\{ s_d \mid s \in S \and s_o = u \right\} \right|
\end{split}
\end{equation}

In particular, Equation~\ref{eq:calls} refers to the amount of in-calls and out-calls each user has, Equation~\ref{eq:time} to the total time in incoming calls and outgoing calls, Equation~\ref{eq:sms} the amount of incoming and outgoing SMS, and Equation~\ref{eq:contacts} to the total amount of different users where either a call of an SMS has been made or received. These last two features correspond to the \emph{In-Degree} and \emph{Out-Degree} of the node in the graph.

These features will be referred as the \emph{User Data} of user $u$.

\subsection{Higher Order User Data}

Where \emph{User Data} contains data directly about the neighbours of each user, we can define \emph{User Data of Order $1$}, as the user data variables about the edges of the neighbours of each user $u$ where $u$ isn't any of the endpoints. In the case of cumulative data (calls, time, and SMS), this is equal to the sum of the \emph{User Data} of the neighbours of $u$ minus the \emph{User Data} os $u$ (since we aren't counting ``inner'' edges). This isn't true in the case of the contacts, since two neighbours of $u$ may have a contact in common, which should be counted only once.

Additionally, for any $n \in \mathbb{N}$, we can inductively define the \emph{User Data of Order $n$} as the user data of the nodes at distance $n$ of a certain node.

\subsection{Categorical User Data}

Another possible featureset consists of using of separating the data of the neighbourhood of each user $u \in U$ into two disjoint groups, $L_u$ and $K_u$, which contain the neighours of $u$ in the low and high categories of income respectively\footnotemark{}.

\begin{equation}
\begin{split}
	L_u &\subseteq H_1 \cap \left( \left\{ p_o \mid p \in P \and p_d = u \right\} \cup \left\{ p_d \mid p \in P \and p_d = u \right\} \right) \\
	K_u &\subseteq H_2 \cap \left( \left\{ p_o \mid p \in P \and p_d = u \right\} \cup \left\{ p_d \mid p \in P \and p_d = u \right\} \right) \\
\end{split}
\end{equation}

Having these groups it's possible to define a set of features similar to the one in Section~\ref{subsec:user_data}, where each feature is separated by the category of the neighbour. Equation~\ref{eq:matcatuserdata} contains the possible forms of the new features, while Equation~\ref{eq:categoricaluserdata} contains the way to calculate some of them. The formula for the rest of the features comes trivially.

\begin{equation}
\label{eq:matcatuserdata}
	\begin{Bmatrix} in \\ out \end{Bmatrix}
	\times
	\begin{Bmatrix} calls \\ time \\ sms \\ contacts \end{Bmatrix}
	\times
	\begin{Bmatrix} low \\ high \end{Bmatrix}
\end{equation}

\begin{equation}
\label{eq:categoricaluserdata}
\begin{split}
	\operatorname{incallslow}_u &= \left| \left\{ p \in P \mid p_d = u \and \mathbf{p_o \in H_1} \right\} \right| \\
	\operatorname{outtimelow}_u &= \sum_{\substack{p \in P \\ p_o = u \\ \mathbf{p_d \in H_1}}} \, p_s \\
	\operatorname{outcontactshigh}_u &= \left| \mathbf{H_2 \, \cap} \, \left( \left\{ p_d \mid p \in P \and p_o = u \right\} \cup \left\{ s_d \mid s \in S \and s_o = u \right\} \right) \right|
\end{split}
\end{equation}

\footnotetext{Note that, since not all users have banking information, there may be nodes in the neighbourhood of $u$ which don't belong to either $L_u$ or $K_u$.}
