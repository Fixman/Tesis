\section{Supervised Machine Learning Models}
\label{subsec:supervised_machine_learning}

This section presents several \emph{Supervised Machine Learning} models that are used in the paper.

Models are separated into two different groups depending on how they describe the input and output variables.

\begin{description}
	\item[Continuous Models] take a matrix $X \in \mathbb{R}^{n \times f}$ and a vector $y \in \mathbb{R}^n$ of the same height where each element represents the features corresponding to some user and its real value, respectively. Its accuracy is measured according to some function of the result of the regression $\ypred$ and $\ytrue = y$.
	\item[Categorical Models] take a matrix $X \in \mathbb{R}^{n \times f}$ and a vector $y \in \mathbb{S}^n$, for some set of \emph{Categories} $\mathbb{S}$, and predicts the correct category each of the users in $y$ belongs to. Its accuracy is measured according to several metrics which are explained in \cref{subsec:mlmetrics}.
\end{description}

\subsection{Linear Regression}
\label{subsec:linearregression}

A \emph{Linear Regression} is an approach for modeling the relationship between the matrix $X$ and the real vector $y$. While it is possible to predict many variables in what's known as the \emph{Multivariate Linear Regression}~\cite{multivariate1979}, this thesis will focus in the single-dimensional variant.

The regression assumes that the relationship between the sum of some linear combination of the elements of $X$ and $y$ is itself linear. This combination is represented by the variable $\beta$, and the relationship is represented through an unobserved random vector $\varepsilon$ as the \emph{Error Term}.

The model takes the form of \cref{eq:linearregression}.

\begin{gather}
	y = X \beta + \varepsilon \label{eq:linearregression} \\
	\intertext{where}
	\begin{gathered}
		X = \begin{pmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_n^T \end{pmatrix} =
		\begin{pmatrix}
			1 & x_{1 1} & \cdots & x_{1 f} \\
			1 & x_{2 1} & \cdots & x_{2 f} \\
			\vdots & \vdots & \ddots & \vdots \\
			1 & x_{n 1} & \cdots & x_{n f} \\
		\end{pmatrix} \\
		y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}, \;
		\beta = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_f \end{pmatrix}, \;
		\varepsilon = \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix}
	\end{gathered}
\end{gather}

$\beta$ is a vector with size $f + 1$, where $\beta_0$ is called the \emph{Constant Term}. The statistical estimation and inference focuses in this variable, as two different models of \emph{Linear Regression} will give different results where this parameter is different~\cite{yan2009linear}.

$\varepsilon_i$ is called the \emph{Error Term} or \emph{Noise}. The variable captures the factors which influence the vector $y$ other than the matrix $X$ and the \emph{Constant Term} $\beta$. The relationship between $\varepsilon$ and those variables and knowing whether they are correlated is important in the formulation of an \emph{Linear Regression} model~\cite{yan2009linear}.

This model makes several assumptions about the data.

\begin{description}
	\item[Weak Exogenety] implies than the variables in $X$ can be treated as fixed values, rather than random variables.
	\item[Linearity] implies that the mean of the response variable is a linear combination of the parameters.
	\item[Homoscedasticity] implies that different response variables have the same \emph{Variance} in their errors. While this is almost never true in practice, since variables tend to vary over a large scale, the data is usually \emph{Standardised} so that this is true.
	\item[Independence of errors] implies that the errors in response variables are uncorrelated with each other, even if they are statistically dependent.
	\item[Lack of multicollinearity] implies that the matrix $X$ must have full column rank; there can't be two perfectly correlated input variables. In this case there won't be a unique solution for the vector $\beta$.
\end{description}

There are many ways of effectively calculating the optimal \emph{Lineal Regression} for a particular pair $\left< X, y \right>$. One of them is using the \textbf{Least Squares Estimation}, which can be solved through the \emph{Least Squares Principle}.

\begin{equation}
\label{eq:leastsquaresprinciple}
	\hat{\beta} = \argmin_\beta \left[ {\left( y - X \beta \right)}^T \left( y - X \beta \right)\right]
\end{equation}

Where $\hat{\beta}^T = \left( b_0, b_1, \dots, b_{k - 1} \right)$, a k-dimensional vector of the estimations of the regression coefficients.

Assuming $\left( X^T X \right)$ is a non-singular matrix, the \emph{Least Squares Estimation} of $\beta$ can for the model in \cref{eq:linearregression} can be found from \cref{eq:linearsquares}~\cite{yan2009linear}.

\begin{equation}
\label{eq:linearsquares}
	\hat{\beta} = {\left( X^T X \right)}^{-1} X^T y
\end{equation}

Additionally, \cref{eq:linearsquares} presents an unbiased estimator of $\beta$~\cite{yan2009linear}.

\subsection{Logistic Regression}
\label{subsec:logisticregression}

The \emph{Logistic Regression}, also referred to as the \emph{Logit Model}~\cite{freedman2009statistical} is a regression model similar to the \emph{Linear Regression}, with the particularity that the \emph{Dependent Variable} $y$ is categorical. While it is possible to calculate the variable for sets of categories of several finite sizes (when it is referred to as a \emph{Multinomial Logistic Regression}~\cite{greene_econometric_2011}) or even for ordered sets of categories (an \emph{Ordinal Logistic Regression}~\cite{mccullagh1980ordinal}), this thesis will focus on the case where $y \in {\left\{ 0, 1 \right\}}^n$, that is, each variable in $y$ is binary.

The model uses the results of a \emph{Linear Regression} on the data $\left< X, y \right>$, where coefficients that solve \cref{eq:linearregression} are found. The result of that regression is a real value, which is normalized using a function $f : \mathbb{R} \rightarrow \left[ 0, 1 \right]$, whose result is considered the probability that the result of some element is $1$.

A commonly used function for $f$ is the \emph{Logistic Function} $\sigma : \mathbb{R} \rightarrow \left[ 0, 1 \right]$, defined in \cref{eq:logisticfunction} and plot in \cref{fig:sigmoid}.

\begin{equation}
\label{eq:logisticfunction}
\sigma \left( t \right) = \frac{e^t}{e^t + 1} = \frac{1}{1 + e^{-t}}
\end{equation}

\begin{figure}
\centering
\includegraphics[height=.33\textheight]{figures/sigmoid.png}
\caption{The standard logistic sigmoid function $y = \sigma(x)$}
\label{fig:sigmoid}
\end{figure}

This probability is used for estimating the value of $y$, namely as defined in \cref{eq:probit}.

\begin{equation}
\label{eq:probit}
\begin{aligned}
	P \left( y_i = 1 \mid X \right) &= \sigma \left( X_i \beta \right) \\
	P \left( y_i = 0 \mid X \right) &= 1 - \sigma \left( X_i \beta \right)
\end{aligned}
\end{equation}

The inverse of the \emph{Logistic Function}, referred to as the \emph{Logit Function}, gives the logarithm of the \emph{Odds} of a certain element belonging to some category.

\begin{equation}
\label{eq:logit}
	\logit \left( x \right) = \log \left( \frac{\sigma \left( x \right)}{1 - \sigma \left( x \right)} \right)
\end{equation}

The logit model says that the vector $y$ is independent given the matrix $X$~\cite{freedman2009statistical}, and can be considered as inverse of the probability defined in \cref{eq:probit}.

\begin{equation}
\label{eq:prob_logit}
	\logit P \left( y_i = 1 \mid X \right) = X_i \beta
\end{equation}

The regression is usually calculated using \emph{Maximum Likelihood Estimation}~\cite{fan2008liblinear} and, unlike the estimation of the \emph{Linear Regression} presented in \cref{subsec:linearregression}, it is not possible to find a closed form expression of the coefficients that maximize the value of the likelihood function. Instead, the iterative \emph{Newton's Method} is used.

The exact formulas used to ensure fast and correct conversed used in this thesis are the ones present in \texttt{LIBLINEAR}, which is presented in~\cite{hsiastudy}.

\subsection{Decision Trees}
\label{subsec:decisiontrees}

A \emph{Decision Tree} is a decision support tool that uses a tree of decisions and their possible consequences, including change event outcomes. They are commonly used in decision analysis to help identify the most optimal strategy to reach a goal.

The tree themselves contain a root at the top, and each non-final node (including the root) contains a binary test of a certain variable; each one of these nodes contains exactly two edges connecting it to the following level, one which is followed in the case the condition is true and another for the case when the condition is false.

\begin{figure}
\centering
\includegraphicsmaybe{figures/decisiontree.png}
\caption{A decision tree. This is a simplified version of one of the decision trees used in the \emph{Random Forest} predictors in \cref{subsec:comparison_random_forest}.}
\label{fig:decision_tree}
\end{figure}

In the context of \emph{Machine Learning}, it can be used as a supervised predictive model which matches properties about the list of features $X$ to the likeliest category $y$~\cite{oded2008decisiontrees}. In the binary case, each of the final nodes contains the output label which, according to the input data, is the most probable given the result of the tests corresponding to the previous nodes to this one along with the probability of this label being true.

There are many algorithms that can generate a tree that's optimized for many different metrics. A common one is the \emph{Classification and Regression Trees (CART)} method, introduced in~\cite{breiman1993classification}, which builds the tree starting from the root and, for each non-leaf node, the variable that generates the best split according to some metric is selected~\cite{loh2011classification}. This is applied recursively until \emph{CART} detects that no further gains can be made, or some pre-set stopping rules are met.

Decision trees are prone to overfitting~\cite{oded2008decisiontrees}. Since a single tree can make an arbitrarily good classification of a set of features despite their properties (unlike models like linear regression, where the data has to be linearly separable), it can lose the capability to generalize for instances not presented in training. This occurs when the tree has too many nodes relative to the amount of training data; \Cref{fig:decisiontreeoverfitting} illustrates the overfitting process.

\begin{figure}
\centering
\includegraphicsmaybe{figures/decisiontreeoverfitting.png}
	\caption{Overfitting in decision trees. When training and testing on the same data, making a classifier more complex makes the accuracy of the prediction increasingly higher until it is pretty much perfect. This result is wrong since the classifier is \emph{Overfitting} its tree to the training data. When testing with different data, it is possible to see how the prediction actually becomes worse. This graph was simulated by training decision trees with data from the \texttt{sklearn.datasets.make\_classification} from the sklearn library\cite{scikit-learn}.}
\label{fig:decisiontreeoverfitting}
\end{figure}

A common mechanism to prevent overfitting in \emph{Decision Trees} is \emph{Pruning}, which reduces the size of decision tree by cutting sections that provide relatively little improvement of the result on the training data.

\subsection{Random Forest}
\label{subsec:randomforest}

\emph{Random Forests} are an ensemble learning method commonly used for classification tasks.
While they use ideas that are similar to the ones used in \cref{subsec:decisiontrees} to build \emph{Decision Trees}, their design prevents them from having problems with overfitting in regards to the training set\cite{rupert2004elements}.

A trained \emph{Random Forest} consists of a set of different \emph{Decision Trees} on the same feature space.
However, these trees should have different biases to compensate for the bias of a single one, since generally using multiple distinct classifiers lowers this value\cite{ho1994decision}.
To achieve these two objectives, each \emph{Decision Tree} is trained on a different subset of the initial feature set\cite{ho1995random}.
Each of these trees classifies the training data with 100\% of accuracy within its feature subspace, yet it generalizes the classification in a different way. 
Since there are $2^m$ possible subspaces for $m$ features, there are many choices in practice.

There are many ways to select which features to use to build each tree.
A simple one is \emph{Bagging} (\textbf{B}ootstrap \textbf{agg}regat\textbf{ing})\cite{breiman1996bagging}, which separates the training set $X$ into $m$ subsets $X'_{1 \dots m}$ uniformly and with replacement.
Later, the $m$ models are fitted using the bootstrap samples, and their output is combined by average the output.

Other methods include \emph{Random Split Selection}, which uses randomization by computing the $k$ (where $k = 20$ in the original paper) top candidate splits, and choosing one of them at random\cite{dietterich2000experimental}.
The best results are seen when using \emph{Adaboost} (for which its authors won the Gödel Prize), a method of \emph{Boosting}, where the features in each tree are selected because of the misclassifications in previous classifiers\cite{freund1996experiments}.

Given the set of features $X \in \mathbb{R}^{n \times f}$ and the set of labels $y \in \left\{ 0, 1\right \}^n$, each iteration of \emph{Adaboost} creates a new decision tree depending on the \emph{Error} of the previous classifiers.
Given a set of already build trees $k_1 \dots k_l$ which output a classification $k_i \left( x_i \right) \in \left\{ 0, 1 \right\}$ for each item, we can define the current random forest as a linear combination of the previous classifiers $C_l$. With these values, we can find the weighted \emph{Error} $E$.

\begin{equation}
	\begin{gathered}
		\begin{aligned}
			C_l \left( x_i \right) &= \sum^l_{i = 1} \alpha_i k_i \left( x_i \right) \\
			C_{l + 1} \left( x_i \right) &= C_l \left( x_i \right) + \alpha_m k_m \left( x_i \right) \\
		\end{aligned} \\
		E = \sum^n_{i = 1} e^{-y_i C_m \left( x_i \right)}
	\end{gathered}
\end{equation}

These values can be later used to improve the current random forest $C_{l + 1}$ that minimizes this error.

Along with \emph{Logistic Regression}, \emph{Random Forests} are one of the learning methods used in the practical parts of this thesis.
In \cref{sec:comparison} it is used to classify the resulting inputs of many feature extraction methods with the \emph{Bayesian Algorithm} introduced in \cref{sec:inference_methodology}. As \cref{subsec:comparison_inner_graph} shows, while the latter algorithm is better suited for our data, using \emph{Random Forest} on a good set of features performs consistently better than other generic \emph{Machine Learning} algorithms.
