\section{Results and Conclusion}
\label{sec:results}

% The results table should have been included earlier, so that the figure can be presented in two columns in this page. Due to limitations on my knowledge of LaTeX, I don't know how to do this inline.

The predictors were run in a computer with a single core of 2.00 GHz Intel Xeon CPU using \texttt{sklearn 0.18} under \texttt{Python 2.7}, and enough RAM to not need caching in any calculationg. The results of the inference can be found in Table~\ref{tab:comparison}.

% We can reach the conclusion than both adding features of a larger \emph{Ego Network} and separating those links via the labels can improve every metric, but the latter features tend to improve them even furtherm, even when taking the entire graph. Additionally, and particularly when comparing predictors with large amount of features, the \emph{Random Forest} tends to be more accurate and much faster than a \emph{Logistic Regression}.

Table~\ref{tab:comparison} shows various metrics which result from applying the methods described in Section~\ref{sec:inference_methodology} with the hyperparameteters that result in the highest \emph{Accuracy} according to the \emph{Grid Search}.

One of the possible conclusions is consistent with~\cite{muchlinski2016}, where the methods based in \emph{Random Forest} tend to perform better in real-world sociolofy scenarios than the ones based in \emph{Logistic Regression}, due to it being more versatile to non-linear the data~\cite{logisticvsdecision}.

Interestingly, increasing the breadth of the \emph{Ego Network} by one level, from $\ego_0$ to $\ego_1$ improves the metrics on these methods when using \emph{Random Forest} learning, however it isn't improved by going one level further to $\ego_2$, despite the data of this being a scrict superset of the previous on to $\ego_2$, despite the data of this being a scrict superset of the previous one. This is because of the fallibility of common \emph{Bagging Methods} like \emph{Random Forest}, where having some noisy or non-informative data to choose from makes it less probable that ``good'' columns will be chosen.

Adding categorical information greatly improves the prediction when using either method, particularly on \emph{Random Forest}, and, like it was noted before, adding neighbouring data of the \emph{Ego Network of Distance 1} also results in a better predictor. However, once again this doesn't happen when raising further the maximum distance within the \emph{Ego Network}.

We can conclude that of all the methods presented, the best one is unambiguously predicting the category using a \emph{Random Forest} using the data from the \emph{Ego Network of Distance 1} ($\cat_1$). While this dataset doesn't result in better methods than $\cat_2$ for users without labelled neighbours, the difference is small enough to be noise, unlike the difference with the users with more neughbouring information.
