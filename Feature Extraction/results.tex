\section{Results and Conclusion}
\label{sec:results}

% The results table should have been included earlier, so that the figure can be presented in two columns in this page. Due to limitations on my knowledge of LaTeX, I don't know how to do this inline.

The predictors were run in a computer with a single core of 2.00 GHz Intel Xeon CPU using \texttt{sklearn 0.18} under \texttt{Python 2.7}, and enough RAM to not need caching in any calculations. The results of the inference can be found in \cref{tab:innergraphresults} for the \emph{Inner Graph} and on \cref{tab:outergraphresults} for the \emph{Full Graph}.

% We can reach the conclusion than both adding features of a larger \emph{Ego Network} and separating those links via the labels can improve every metric, but the latter features tend to improve them even furtherm, even when taking the entire graph. Additionally, and particularly when comparing predictors with large amount of features, the \emph{Random Forest} tends to be more accurate and much faster than a \emph{Logistic Regression}.

Both tables show various metrics which result from applying the methods described in \cref{sec:inference_methodology} with the hyperparameters that result in the highest \emph{Accuracy} according to the \emph{Grid Search}.

One of the possible conclusions is that methods based in \emph{Random Forest} tend to perform better in real-world sociology scenarios than the ones based in \emph{Logistic Regression}, due to it being more versatile to non-linear the data~\cite{logisticvsdecision}. This is consistent with similar findings in~\cite{muchlinski2016}.

Interestingly, increasing the breadth of the \emph{Ego Network} by one level, from $\ego_1$ to $\ego_2$ improves the metrics on these methods when using \emph{Random Forest} learning, however it isn't improved by going one level further to $\ego_3$, despite the data of this being a strict superset of the previous on to $\ego_3$. This is because of the fallibility of common \emph{Bagging Methods} like \emph{Random Forest}, where having some noisy or non-informative data to choose from makes it less probable that informative features will be chosen.

Adding categorical information greatly improves the prediction when using either method, particularly on \emph{Random Forest}, and, like it was noted before, adding neighbouring data of the \emph{Ego Network of Distance 1} also results in a better predictor. However, once again this doesn't happen when raising further the maximum distance within the \emph{Ego Network}.

We can conclude that of all the methods presented, the best one is unambiguously predicting the category using a \emph{Random Forest} using the data from the \emph{Ego Network of Distance 1} ($\cat_2$). While this dataset doesn't result in better methods than $\cat_3$ for users without labelled neighbours, the difference is small enough to be noise, unlike the difference with the users with more neighbouring information.

In the \emph{Inner Graph} the best method is the \emph{Bayesian Method} presented in~\cite{fixmanasonam2016}, which only uses the amount of \emph{High Income} and \emph{Low Income}, but makes a ``smarter'' prediction than the \emph{Machine Learning} methods using the models $\cat_1$, $\cat_2$, and $\cat_3$, which also contain this data. Additionally, while its \emph{Area Under the Curve} isn't higher than in the best \emph{Machine Learning} methods, the \emph{F\textsubscript{1}-score} of the prediction which uses the method \emph{Majority Voting} is higher than in all the \emph{Machine Learning} methods.

We can reach the conclusion that, in this particular case, \emph{smaller is better}. The \emph{Machine Learning} methods which use many features (despite these features being informative) aren't better at predicting the \emph{Socioeconomic Index} of a user than the simple \emph{Majority Voting} or the more complicated \emph{Bayesian Method} which use only 2 simple ones.
